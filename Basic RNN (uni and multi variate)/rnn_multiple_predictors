{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rnn_multiple_predictors","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPu4QTru4R4OTJCJUfg1lxS"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"g3qTo2a_aUF5"},"source":["# Data cleaning"]},{"cell_type":"code","metadata":{"id":"EztE3hbsXhyO","executionInfo":{"status":"ok","timestamp":1602430931689,"user_tz":-120,"elapsed":774,"user":{"displayName":"Tidely Pom","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir7f-K_PBvQWDiuGzlR22Xx10yMb5xYPmopw7fvw=s64","userId":"06149365859867560433"}}},"source":["#Cleaning data\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"BbZxOJxPoTGA","executionInfo":{"status":"ok","timestamp":1602425898760,"user_tz":-120,"elapsed":808,"user":{"displayName":"Tidely Pom","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir7f-K_PBvQWDiuGzlR22Xx10yMb5xYPmopw7fvw=s64","userId":"06149365859867560433"}},"outputId":"591955b9-a807-498b-b83e-31c2f30fb193","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import os \n","os.getcwd()"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"pUct5Q3TaY-J","executionInfo":{"status":"ok","timestamp":1602430936084,"user_tz":-120,"elapsed":3440,"user":{"displayName":"Tidely Pom","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir7f-K_PBvQWDiuGzlR22Xx10yMb5xYPmopw7fvw=s64","userId":"06149365859867560433"}}},"source":["# Importing Training Set\n","raw_dataset_rainfall= pd.read_csv('Rainfall_zwolle.csv')\n","\n","raw_dataset_no2 = pd.read_csv('no2.csv')\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3SGGnMiSSdX","executionInfo":{"status":"ok","timestamp":1602431003787,"user_tz":-120,"elapsed":705,"user":{"displayName":"Tidely Pom","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir7f-K_PBvQWDiuGzlR22Xx10yMb5xYPmopw7fvw=s64","userId":"06149365859867560433"}},"outputId":"eb6c8234-b6a0-45e4-e715-775631eac34c","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["##remove unneccassary columns\n","col_to_remove_rainfall = ('X','Y', 'Height', 'StationName', 'ReportEndDateTime', 'ExternalSiteId', 'RainfallRate_MillimetrePerHour', 'OBJECTID' )\n","\n","for name in col_to_remove_rainfall:\n","  if name in raw_dataset_rainfall.columns:\n","    raw_dataset_rainfall = raw_dataset_rainfall.drop(name,1)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.indexes.base.Index'>\n","<class 'pandas.core.indexes.base.Index'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8xwzvL-fb11R"},"source":["col_to_remove_no2 =('wkt_geom', 'id', 'timestam_1', 'unit_NO2', 'unit_PM10', 'unit_RH', 'unit_P', 'value_PM10', 'value_RH', 'value_P')\n","for name in col_to_remove_no2:\n","  if name in raw_dataset_no2.columns:\n","    raw_dataset_no2 = raw_dataset_no2.drop(name,1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cXEvSq9KVmKQ"},"source":[""]},{"cell_type":"code","metadata":{"id":"nXUfENT9ZlOD","executionInfo":{"status":"ok","timestamp":1602434539601,"user_tz":-120,"elapsed":2798,"user":{"displayName":"Tidely Pom","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir7f-K_PBvQWDiuGzlR22Xx10yMb5xYPmopw7fvw=s64","userId":"06149365859867560433"}}},"source":["##creating list of dataframes for each rainfall sensor\n","rainfall_sensors = raw_dataset_rainfall.SiteID.unique()\n","dataset_per_r_sensor =[] #list of dataframes for each rainfall sensor\n","\n","count=0\n","while count <=len(rainfall_sensors):\n","  for sensor in rainfall_sensors:\n","    dataset_per_r_sensor.append(raw_dataset_rainfall.loc[raw_dataset_rainfall['SiteID'] == sensor])\n","    dataset_per_r_sensor[count].sort_values('ReportStartDateTime' , ascending=True)\n","    count+=1\n","\n","##remove rows of first date \n","  ##in order to start from 00:00\n","  ##should be 144 records per day\n","for dataset in dataset_per_r_sensor:\n","  min_date = dataset.ReportStartDateTime.min()\n","  dataset=dataset[dataset.ReportStartDateTime != min_date]\n"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"PylXvzRrZm9h","executionInfo":{"status":"ok","timestamp":1602434084152,"user_tz":-120,"elapsed":1217,"user":{"displayName":"Tidely Pom","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gir7f-K_PBvQWDiuGzlR22Xx10yMb5xYPmopw7fvw=s64","userId":"06149365859867560433"}}},"source":["##creating list of dataframes for each no2 sensor\n","no2_sensors = raw_dataset_no2.label.unique()\n","dataset_per_no2_sensor =[] #list of dataframes for each NO2 sensor\n","\n","count=0\n","while count <=len(no2_sensors):\n","  for sensor in no2_sensors:\n","    dataset_per_no2_sensor.append(raw_dataset_no2.loc[raw_dataset_no2['label'] == sensor])\n","    dataset_per_no2_sensor[count].sort_values('timestamp_' , ascending=True)\n","    count+=1\n","\n","##remove rows for first date\n","  ##in order to start from 00:00\n","  ##should be 24 records per date\n","for dataset in dataset_per_no2_sensor:\n","  min_date = dataset.timestamp_.min()\n","  dataset=dataset[dataset.timestamp_ != min_date]\n","\n"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"88JUKArYg-tq"},"source":["##selecting rainfall and no2 sensors that are in close proximity and combining their datasets\n","rainfall_no2_sensor_pair = [tuples]\n","for dataset in dataset_per_r_sensor:\n","  for dataframe in dataset_per_no2_sensor:\n","\n","##make sure dates are matching\n","##remove dates that are not present in no2 data\n","##if not genrate no2 data to match number of rainfall data records"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KkOlVbD4ccLj"},"source":["\n","\n","##splitting each remaining dataset into train/test\n","  ##train_x n test_x == rainfall dataset (train proportion)\n","  ##train_y and test_y == no2 dataset (test proportion)\n","dataset_train=[]\n","dataset_test=[]\n","dataset_train_x = [] #list of dataframes for each sensor for training\n","dataset_train_y=[]\n","\n","dataset_test_x = [] #list of dataframes for each sensor for testing\n","dataset_test_y = []\n","\n","##Splitting each rainfall dataset into train and test sets\n","train_split_ratio = 0.8 #select train split ratio\n","for dataset in dataset_per_r_sensor:\n","  train_len = int(train_split_ratio*len(dataset))\n","  test_len = len(dataset) - train_len\n","\n","  dataset_train_sensors.append(dataset[:train_len])\n","  dataset_test_sensors.append(dataset[train_len:-1])\n","\n","\n","\n","\n","\n","\n","        \n","\n","    \n","##Extract columns required for feature training and for predictions\n","\n","\n","\n","#cols = list(dataset_train)[1:5]\n","#dataset_train_sensors = dataset_train_sensors[cols]\n","#dataset_test_sensors = dataset_test_sensors[cols]\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BPoL_NjVbBzn"},"source":["##average out nan values using a window/kernel\n","#this might take a minute\n","\n","window_size = 20 #select a window size to calculate average values for cells with no data\n","for dataset in dataset_train_sensors:\n","\n","  \n","  for c in range(len(dataset.columns)):\n","    for r in range(len(dataset)):\n","      if dataset.iloc[r,c]== np.nan:\n","\n","        window_pos = range(window_size)\n","        window_neg = [ -x for x in window_pos]\n","        window = window_pos+ window_neg\n","\n","        mean_values=[]\n","        for w in window:\n","          if dataset.iloc[r+w,c] >=0:\n","            mean_values.append(dataset.iloc[r+w,c])\n","        mean = (sum(mean_values))/mean_values.count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXDtUZ3GEara"},"source":["##scale data\n","from sklearn.preprocessing import StandardScaler\n"," \n","sc = StandardScaler()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ba7YTJBjaaKN"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"vZ9AVNh3GKS_"},"source":["# Part 1 - Data Preprocessing\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n"," \n","\n","#Preprocess data for training by removing all commas\n"," \n","dataset_train = dataset_train[cols].astype(str)\n","for i in cols:\n","    for j in range(0,len(dataset_train)):\n","        dataset_train[i][j] = dataset_train[i][j].replace(\",\",\"\")\n"," \n","dataset_train = dataset_train.astype(float)\n"," \n"," \n","training_set = dataset_train.as_matrix() # Using multiple predictors.\n"," \n","# Feature Scaling\n","from sklearn.preprocessing import StandardScaler\n"," \n","sc = StandardScaler()\n","training_set_scaled = sc.fit_transform(training_set)\n"," \n","sc_predict = StandardScaler()\n"," \n","sc_predict.fit_transform(training_set[:,0:1])\n"," \n","# Creating a data structure with 60 timesteps and 1 output\n","X_train = []\n","y_train = []\n"," \n","n_future = 20  # Number of days you want to predict into the future\n","n_past = 60  # Number of past days you want to use to predict the future\n"," \n","for i in range(n_past, len(training_set_scaled) - n_future + 1):\n","    X_train.append(training_set_scaled[i - n_past:i, 0:5])\n","    y_train.append(training_set_scaled[i+n_future-1:i + n_future, 0])\n"," \n","X_train, y_train = np.array(X_train), np.array(y_train)\n"," \n","# Part 2 - Building the RNN\n"," \n","# Import Libraries and packages from Keras\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Dropout\n"," \n","# Initializing the RNN\n","regressor = Sequential()\n"," \n","# Adding fist LSTM layer and Drop out Regularization\n","regressor.add(LSTM(units=10, return_sequences=True, input_shape=(n_past, 4)))\n"," \n"," \n","# Part 3 - Adding more layers\n","# Adding 2nd layer with some drop out regularization\n","regressor.add(LSTM(units=4, return_sequences=False))\n"," \n","# Output layer\n","regressor.add(Dense(units=1, activation='linear'))\n"," \n","# Compiling the RNN\n","regressor.compile(optimizer='adam', loss=\"mean_squared_error\")  # Can change loss to mean-squared-error if you require.\n"," \n","# Fitting RNN to training set using Keras Callbacks. Read Keras callbacks docs for more info.\n"," \n","es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n","rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n","mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n","tb = TensorBoard('logs')\n"," \n","history = regressor.fit(X_train, y_train, shuffle=True, epochs=100,\n","                        callbacks=[es, rlr,mcp, tb], validation_split=0.2, verbose=1, batch_size=64)\n"," \n"," \n","# Predicting the future.\n","#--------------------------------------------------------\n","# The last date for our training set is 30-Dec-2016.\n","# Lets now try predicting the stocks for the dates in the test set.\n"," \n","# The dates on our test set are:\n","# 3,4,5,6,9,10,11,12,13,17,18,19,20,23,24,25,26,27,30,31-Jan-2017\n"," \n","# Now, the latest we can predict into our test set is to the 19th since the last date on training is 30-Dec-2016. \n","# 20 days into the future from the latest day in our training set is 19-Dec-2016. Right?\n","# Notice that we dont have some days in our test set, what we can do is to take the last 20 samples from the training set. \n","# (Remember the last sample of our training set will predict the 19th of Jan 2017, the second last will predict the 18th, etc)\n"," \n"," \n","# Lets first import the test_set.\n","dataset_test = pd.read_csv('Google_Stock_Price_Test.csv')\n","y_true = np.array(dataset_test['Open'])\n","#Trim the test set to first 12 entries (till the 19th)\n","y_true = y_true[0:12]\n","predictions = regressor.predict(X_train[-20:])\n"," \n"," \n","# We skip the 31-Dec, 1-Jan,2-Jan, etc to compare with the test_set\n","predictions_to_compare = predictions[[3,4,5,6,9,10,11,12,13,17,18,19]]\n","y_pred = sc_predict.inverse_transform(predictions_to_compare)\n"," \n"," \n"," \n","hfm, = plt.plot(y_pred, 'r', label='predicted_stock_price')\n","hfm2, = plt.plot(y_true,'b', label = 'actual_stock_price')\n"," \n","plt.legend(handles=[hfm,hfm2])\n","plt.title('Predictions and Actual Price')\n","plt.xlabel('Sample index')\n","plt.ylabel('Stock Price Future')\n","plt.savefig('graph.png', bbox_inches='tight')\n","plt.show()\n","plt.close()\n"," \n"," \n"," \n","hfm, = plt.plot(sc_predict.inverse_transform(y_train), 'r', label='actual_training_stock_price')\n","hfm2, = plt.plot(sc_predict.inverse_transform(regressor.predict(X_train)),'b', label = 'predicted_training_stock_price')\n"," \n","plt.legend(handles=[hfm,hfm2])\n","plt.title('Predictions vs Actual Price')\n","plt.xlabel('Sample index')\n","plt.ylabel('Stock Price Training')\n","plt.savefig('graph_training.png', bbox_inches='tight')\n","plt.show()\n","plt.close()\n"," "],"execution_count":null,"outputs":[]}]}